{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d46aa8a",
   "metadata": {},
   "source": [
    "### Import Libraries and IBM PAIRS Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f50cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ibmpairs import paw, authentication\n",
    "import json\n",
    "import numpy\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import traceback\n",
    "import rasterio\n",
    "import os\n",
    "import zipfile\n",
    "import traceback\n",
    "import zipfile\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dabf478",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "pawLogger = logging.getLogger('ibmpairs.paw')\n",
    "pawLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd18804",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ibmpairskey.txt\", \"r\") as f:\n",
    "    pairs_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc990dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate(pairs_key):\n",
    "    \n",
    "    pairs_credentials = authentication.OAuth2(api_key=pairs_key)\n",
    "    auth_header = {'Authorization': f'Bearer {pairs_credentials.jwt_token}'}\n",
    "    PAIRS_SERVER   = 'https://pairs.res.ibm.com'\n",
    "    PAIRS_CREDENTIALS = authentication.OAuth2(api_key=pairs_key)\n",
    "    \n",
    "    return PAIRS_SERVER, PAIRS_CREDENTIALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8381bbf",
   "metadata": {},
   "source": [
    "### Define Parameters\n",
    "Define bbox coordinates,start and end dates, query structure, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1db794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel 2 Layers\n",
    "senLayers = {\n",
    "    \"Red\" : \"49360\",\n",
    "    \"Green\" : \"49681\",\n",
    "    \"Blue\" : \"49680\",\n",
    "    \"NIR\": \"49361\",\n",
    "    \"NDVI\" : \"49464\",\n",
    "    \"SCL\": \"49362\",\n",
    "    \"B8a\": \"49685\", #Narrow IR\n",
    "    \"B12\": \"49687\", # SWIR 2200 nm\n",
    "    \"B11\" : \"49686\", # SWIR 1610 nm\n",
    "    \"B5\" : \"49682\", # vegetation red edge\n",
    "    \"B6\" : \"49683\", # vegetation red edge\n",
    "    \"B7\" : \"49684\", # vegetation red edge\n",
    "    \"CP\" : \"50250\" # cloud prob map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88134c30-70c2-44ba-988c-8e59855e4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra derived layers for Sentinel 2\n",
    "\n",
    "index_layers = [\n",
    "    {\n",
    "    \"alias\": \"NDWI\",\n",
    "    \"expression\": \"($Green - $NIR)/($Green + $NIR)\"\n",
    "    },\n",
    "    {\n",
    "    \"alias\": \"NDMI\",\n",
    "    \"expression\": \"($NIR - $B11)/($NIR + $B11)\"\n",
    "    }, \n",
    "    {\n",
    "    \"alias\": \"SAVI\",\n",
    "    \"expression\": \"($NIR - $Red)/(($NIR + $Red + 0.5)*(1.0 + 0.5))\"\n",
    "    }, \n",
    "    {\n",
    "    \"alias\": \"ARVI\",\n",
    "    \"expression\": \"($B8a - $Red -0.106*($Red - $Blue))/($B8a +$Red - 0.106*($Red - $Blue))\"\n",
    "    },\n",
    "    {\n",
    "    \"alias\": \"MSI\",\n",
    "    \"expression\": \"$B11/$B8a\"\n",
    "    },\n",
    "    {\n",
    "    \"alias\": \"NDVIre\",\n",
    "    \"expression\": \"($B8a-$B5)/($B8a+$B5)\"\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31e7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA Layers\n",
    "eraLayers = {\n",
    "    \"Red\" : \"49360\",\n",
    "    \"Total Prec\" : \"49459\",\n",
    "    \"SP\" : \"49439\",\n",
    "    \"Temp\" : \"49423\",\n",
    "    \"AWVC\" : \"49458\",\n",
    "    #\"PrecType\" : \"49435\", #skip for now since mode aggregation is not possible\n",
    "    \"SR\" : \"49440\",\n",
    "    \"TCC\" : \"49454\",\n",
    "    \"MinT\" : \"49429\",\n",
    "    \"MaxT\" : \"49430\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537037b5-83a6-459e-bd9f-4118a1bf22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Beni's method (approach 1)\n",
    "\n",
    "vpd_layers = [\n",
    "        {\n",
    "            #convert kelvin to celcius\n",
    "            \"alias\": \"tmaxc\",\n",
    "            \"expression\": \"$MaxT - 273.15\",\n",
    "             \"output\" : \"false\"\n",
    "        },\n",
    "        {            \n",
    "            # Min Rel humidity\n",
    "            \"type\" : \"raster\", \"id\" : \"48760\",\n",
    "            \"aggregation\" : \"Min\",\n",
    "            \"alias\": \"relativehum\",\n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {\n",
    "            # calculate saturation water vapour pressure in Pa\n",
    "            \"alias\": \"esat\",\n",
    "            \"expression\": \"611*math:exp((17.27 * $tmaxc)/($tmaxc + 237.3))\",\n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {\n",
    "            # Calculate vpd\n",
    "            \"alias\": \"vpd\",\n",
    "            \"expression\": \"$esat * (1 - $relativehum)\" #,\n",
    "            #\"output\" : \"false\"\n",
    "        },\n",
    "                {\n",
    "            # Calculate vpd\n",
    "            \"alias\": \"vpd_final\",\n",
    "            \"expression\": \"math:max(0, $vpd)\"\n",
    "        }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7eae9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date and space parameters\n",
    "date_start, date_end = None, None #Needed for defining the query for ERA5\n",
    "date = None #Needed for defining the query for Sentinel 2\n",
    "bbox = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4e6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define areas\n",
    "areas = {\n",
    "    'bbox_1' : [\"46.94599\", \"7.68597\", \"47.20599\", \"7.94597\"]\n",
    "    #'bbox_2' : [\"47.20599\",\"7.94597\",\"47.46599\",\"8.20597\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8164f320-2bb8-4438-b527-963063122a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the max area\n",
    "\n",
    "areas = {\n",
    "    'bbox_1' : [\"47.1\", \"6.9\", \"47.15\", \"7.95\"]\n",
    "    #'bbox_2' : [\"47.20599\",\"7.94597\",\"47.46599\",\"8.20597\"]\n",
    "}\n",
    "\n",
    "# 0.9 deg = 8.8 GB for Sen2, 7.5 GB for ERA5\n",
    "# 1 deg = 10.9 GB for Sen2, 8.84 for ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9adcc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query for Sentinel 2\n",
    "querySentinelJson = {\n",
    "    \"layers\" : [\n",
    "        {\n",
    "            \n",
    "            \"type\" : \"raster\", \"id\" : senLayers[lKey], \"alias\": lKey\n",
    "        }\n",
    "        for lKey in senLayers\n",
    "    ] + index_layers,\n",
    "    \"spatial\" : {\"type\" : \"square\",  \"coordinates\" : bbox }, \n",
    "    \"temporal\" : {\"intervals\" : [{\"snapshot\" : date}]},\n",
    "    \"processor\" : [{\n",
    "        \"order\" : 1,\n",
    "        \"type\" : \"coarse-grain\",\n",
    "        \"options\" : [\n",
    "            {\"name\" : \"levelsUp\", \"value\" : \"2\"},\n",
    "            {\"name\" : \"aggregation\", \"value\" : \"bilinear\"}\n",
    "        ]\n",
    "    }] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91b77623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query for ERA5\n",
    "queryEraJson = {\n",
    "    \"layers\" : [\n",
    "        {            \n",
    "            \"type\" : \"raster\", \"id\" : eraLayers[lKey], \"alias\": lKey,\n",
    "            \"aggregation\" : None\n",
    "        }\n",
    "        for lKey in eraLayers    \n",
    "    ] + vpd_layers,\n",
    "    \"spatial\" : {\"type\" : \"square\",  \"coordinates\" : bbox }, \n",
    "    \"temporal\" : {\"intervals\" : [\n",
    "      {\"start\" : date_start, \"end\" : date_end}\n",
    "  ]},\n",
    "    \"processor\" : [{\n",
    "        \"order\" : 1,\n",
    "        \"type\" : \"coarse-grain\",\n",
    "        \"options\" : [\n",
    "            {\"name\" : \"levelsUp\", \"value\" : \"2\"},\n",
    "            {\"name\" : \"aggregation\", \"value\" : \"bilinear\"}\n",
    "        ]\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe5e1d04-810d-4077-a776-9ded63c66832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define   aggregation type for each ERA 5 layer\n",
    "agg_mean = [\"49360\", \"49439\", \"49454\", \"49423\", \"49458\"] #Red, Surface pressure, Total cloud cover, Temperature, AWVC\n",
    "agg_sum = [\"49459\", \"49440\"] # Total precipitation, Solar radiation\n",
    "agg_max = [\"49430\"] # Max temperature\n",
    "agg_min = [\"49429\"] # Min temperature\n",
    "#agg_mode = [\"49435\"] # Prec type\n",
    "# still need to compute Vapor Pressure Deficit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03f6108d-1026-4b44-9e4e-4c81ba819956",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, layer in enumerate(queryEraJson[\"layers\"]):\n",
    "    if \"id\" in layer:\n",
    "        if layer[\"id\"] in agg_mean:\n",
    "            queryEraJson[\"layers\"][idx][\"aggregation\"] = 'Mean' #'Median' doesnt exist\n",
    "        if layer[\"id\"] in agg_sum:\n",
    "            queryEraJson[\"layers\"][idx][\"aggregation\"] = 'Sum'\n",
    "        if layer[\"id\"] in agg_max:\n",
    "            queryEraJson[\"layers\"][idx][\"aggregation\"] = 'Max'\n",
    "        if layer[\"id\"] in agg_min:\n",
    "            queryEraJson[\"layers\"][idx][\"aggregation\"] = 'Min'\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e64ef",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "Loop through grid coordinates and only download ERA5 when Sentinel 2 is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1131bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that sets the date range\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1b6bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "# Define start and end date\n",
    "start_date = date(2019, 4, 5)\n",
    "end_date = date(2019, 4, 6)\n",
    "\n",
    "iso8601 = '%Y-%m-%dT%H:%M:%SZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a73ca973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying for date 2019-04-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ibmpairs.paw:PAIRS query JSON initialized.\n",
      "WARNING:ibmpairs.paw:Fake submit to PAIRS in order to use (latest) locally cached data.\n",
      "WARNING:ibmpairs.paw:I am sorry, you asked for using the local cache, but your query does not match any existing. I am gonna query PAIRS instead: 'list index out of range'.\n",
      "INFO:ibmpairs.paw:Query successfully submitted, reference ID: 1666756800_27165169\n",
      "INFO:ibmpairs.paw:Here we go, PAIRS query result successfully downloaded as '1666756800_27165169_4823d16415989c29e3196162fd201669.zip'.\n",
      "INFO:ibmpairs.paw:Data acknowledgement successfully loaded, print with `self.print_data_acknowledgement()`\n",
      "INFO:ibmpairs.paw:PAIRS meta data loaded from 'output.info'.\n",
      "INFO:ibmpairs.paw:PAIRS query JSON initialized.\n",
      "WARNING:ibmpairs.paw:Fake submit to PAIRS in order to use (latest) locally cached data.\n",
      "WARNING:ibmpairs.paw:I am sorry, you asked for using the local cache, but your query does not match any existing. I am gonna query PAIRS instead: 'list index out of range'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentinel-2 data downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ibmpairs.paw:Query successfully submitted, reference ID: 1666756800_27776641\n",
      "INFO:ibmpairs.paw:Here we go, PAIRS query result successfully downloaded as '1666756800_27776641_476182c307777bfeb74ac907675f7874.zip'.\n",
      "INFO:ibmpairs.paw:Data acknowledgement successfully loaded, print with `self.print_data_acknowledgement()`\n",
      "INFO:ibmpairs.paw:PAIRS meta data loaded from 'output.info'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERA-5 data downloaded\n"
     ]
    }
   ],
   "source": [
    "# Authenticate before every query because of the time-out problem\n",
    "for bbox, coord in areas.items():\n",
    "    \n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        print(f'Trying for date {single_date}')\n",
    "        PAIRS_SERVER, PAIRS_CREDENTIALS = authenticate(pairs_key)\n",
    "    \n",
    "        # Try Sentinel 2 first\n",
    "    \n",
    "        date = single_date.strftime(iso8601)\n",
    "        querySentinelJson[\"temporal\"][\"intervals\"][0][\"snapshot\"] = date\n",
    "        querySentinelJson[\"spatial\"][\"coordinates\"] = coord\n",
    "        sentinel_check = 0 # to check if there is Sentinel 2 data for this date or not\n",
    "        \n",
    "        try:    \n",
    "            querySentinel = paw.PAIRSQuery(querySentinelJson, PAIRS_SERVER, PAIRS_CREDENTIALS, authType='api-key', overwriteExisting=False)\n",
    "            querySentinel.submit()\n",
    "        \n",
    "            querySentinel.poll_till_finished()\n",
    "            querySentinel.download()\n",
    "            querySentinel.create_layers()\n",
    "            \n",
    "            #Check if the Sentinel folder exists\n",
    "            path_sentinel = 'downloads/SENTINEL 2/'\n",
    "            isExistSentinel = os.path.exists(path_sentinel)                          \n",
    "            \n",
    "            if isExistSentinel == True : \n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(path_sentinel)\n",
    "                \n",
    "            \n",
    "            #Rename File\n",
    "            old_name = 'downloads/' + str(querySentinel.zipFilePath)\n",
    "            new_name = 'downloads/SENTINEL 2/' + str(date) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3]) + '.zip'\n",
    "            os.rename(old_name, new_name)\n",
    "            \n",
    "            #extract zip file\n",
    "            directory_name = 'downloads/SENTINEL 2/' + str(date) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3])\n",
    "            with zipfile.ZipFile(new_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(directory_name)\n",
    "            os.remove(new_name)\n",
    "            \n",
    "            #remove original files\n",
    "            for f in os.listdir(directory_name + \"/original/\"):\n",
    "                os.remove(directory_name + \"/original/\" + f)\n",
    "            os.rmdir(directory_name + \"/original/\")\n",
    "            \n",
    "            print('Sentinel-2 data downloaded')\n",
    "            \n",
    "            # Could add some data manipulation here and then save the data\n",
    "            sentinel_check = 1\n",
    "        except:\n",
    "            print('No Sentinel-2 data for this date') \n",
    "       \n",
    "    \n",
    "        if sentinel_check:\n",
    "            # Now download ERA 5 for dates of Sentinel 2\n",
    "            date_start = single_date.strftime(iso8601)\n",
    "            date_end = (single_date + timedelta(1)).strftime(iso8601)\n",
    "            queryEraJson[\"temporal\"][\"intervals\"][0][\"start\"] = date_start\n",
    "            queryEraJson[\"temporal\"][\"intervals\"][0][\"end\"] = date_end\n",
    "            queryEraJson[\"spatial\"][\"coordinates\"] = coord\n",
    "\n",
    "\n",
    "            try:    \n",
    "                queryEra = paw.PAIRSQuery(queryEraJson, PAIRS_SERVER, PAIRS_CREDENTIALS, authType='api-key', overwriteExisting=False)\n",
    "                queryEra.submit()\n",
    "\n",
    "                queryEra.poll_till_finished()\n",
    "                queryEra.download()\n",
    "                queryEra.create_layers()\n",
    "                \n",
    "                #Check if the ERA folder exists\n",
    "                path_era = 'downloads/ERA5/'\n",
    "                isExistEra = os.path.exists(path_era)                          \n",
    "            \n",
    "                if isExistEra == True : \n",
    "                    pass\n",
    "                else:\n",
    "                    os.makedirs(path_era)\n",
    "                \n",
    "                #Rename File\n",
    "                old_name = 'downloads/' + str(queryEra.zipFilePath)\n",
    "                new_name = 'downloads/ERA5/' + str(date) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3]) + '.zip'\n",
    "                os.rename(old_name, new_name)\n",
    "                \n",
    "                        \n",
    "                # Delete Sentinel 2 files in ERA folder            \n",
    "                #extract zip file\n",
    "                directory_name = 'downloads/ERA5/' + str(date) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3])\n",
    "                with zipfile.ZipFile(new_name, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(directory_name)\n",
    "\n",
    "                #delete the zip file\n",
    "                os.remove(new_name)\n",
    "\n",
    "                #remove Sentinel 2 bands from the directory\n",
    "                for f in glob.glob(directory_name + \"/High*.*\"):\n",
    "                    os.remove(f)\n",
    "                #remove original files\n",
    "                for f in os.listdir(directory_name + \"/original/\"):\n",
    "                    os.remove(directory_name + \"/original/\" + f)\n",
    "                os.rmdir(directory_name + \"/original/\")\n",
    "                \n",
    "                print('ERA-5 data downloaded')\n",
    "                \n",
    "                \n",
    "            except:\n",
    "                print('No ERA 5 data for this date')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1662cd-89d2-4274-9624-bde26754aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOADING ALL DATES ERA 5: add red to both Sen2 and ERA5\n",
    "\n",
    "# Authenticate before every query because of the time-out problem\n",
    "for bbox, coord in areas.items():\n",
    "    \n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        print(f'Trying for date {single_date}')\n",
    "        PAIRS_SERVER, PAIRS_CREDENTIALS = authenticate(pairs_key)\n",
    "    \n",
    "        # Try Sentinel 2 first\n",
    "    \n",
    "        date = single_date.strftime(iso8601)\n",
    "        querySentinelJson[\"temporal\"][\"intervals\"][0][\"snapshot\"] = date\n",
    "        querySentinelJson[\"spatial\"][\"coordinates\"] = coord\n",
    "        sentinel_check = 0 # to check if there is Sentinel 2 data for this date or not\n",
    "    \n",
    "        try:   \n",
    "            print(f'Trying Sentinel...')\n",
    "            querySentinel = paw.PAIRSQuery(querySentinelJson, PAIRS_SERVER, PAIRS_CREDENTIALS, authType='api-key', overwriteExisting=False)\n",
    "            querySentinel.submit()\n",
    "        \n",
    "            querySentinel.poll_till_finished()\n",
    "            querySentinel.download()\n",
    "            querySentinel.create_layers()\n",
    "            \n",
    "            #Check if the Sentinel folder exists\n",
    "            path_sentinel = 'downloads/SENTINEL 2/'\n",
    "            isExistSentinel = os.path.exists(path_sentinel)                          \n",
    "            \n",
    "            if isExistSentinel == True : \n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(path_sentinel)\n",
    "                \n",
    "            \n",
    "            #remove original files\n",
    "                for f in os.listdir(directory_name + \"/original/\"):\n",
    "                    os.remove(f)\n",
    "                os.rmdir(directory_name + \"/original/\")\n",
    "            \n",
    "            # Could add some data manipulation here and then save the data\n",
    "            sentinel_check = 1\n",
    "        except:\n",
    "            print('No Sentinel-2 data for this date')\n",
    "        \n",
    "    \n",
    "\n",
    "        # Now download ERA 5\n",
    "        date_start = single_date.strftime(iso8601)\n",
    "        date_end = (single_date + timedelta(1)).strftime(iso8601)\n",
    "        queryEraJson[\"temporal\"][\"intervals\"][0][\"start\"] = date_start\n",
    "        queryEraJson[\"temporal\"][\"intervals\"][0][\"end\"] = date_end\n",
    "        queryEraJson[\"spatial\"][\"coordinates\"] = coord\n",
    "        \n",
    "    \n",
    "\n",
    "        try:   \n",
    "            print(f'Trying ERA...')\n",
    "            queryEra = paw.PAIRSQuery(queryEraJson, PAIRS_SERVER, PAIRS_CREDENTIALS, authType='api-key', overwriteExisting=False)\n",
    "            queryEra.submit()\n",
    "\n",
    "            queryEra.poll_till_finished()\n",
    "            queryEra.download()\n",
    "            queryEra.create_layers()\n",
    "\n",
    "            #Check if the ERA folder exists\n",
    "            path_era = 'downloads/ERA5/'\n",
    "            isExistEra = os.path.exists(path_era)                          \n",
    "\n",
    "            if isExistEra == True : \n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(path_era)\n",
    "\n",
    "            #Rename File\n",
    "            old_name = 'downloads/' + str(queryEra.zipFilePath)\n",
    "            new_name = 'downloads/ERA5/' + str(date) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3]) + '.zip'\n",
    "            os.rename(old_name, new_name)\n",
    "\n",
    "            # Delete Sentinel 2 files             \n",
    "            #extract zip file\n",
    "            directory_name = 'downloads/ERA5/' + str(date) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3])\n",
    "            with zipfile.ZipFile(new_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(directory_name)\n",
    "\n",
    "            #delete the zip file\n",
    "            os.remove(new_name)\n",
    "\n",
    "            #remove Sentinel 2 bands from the directory\n",
    "            for f in glob.glob(directory_name + \"/High*.*\"):\n",
    "                os.remove(f)\n",
    "            #remove original files\n",
    "            for f in os.listdir(directory_name + \"/original/\"):\n",
    "                os.remove(f)\n",
    "            os.rmdir(directory_name + \"/original/\")\n",
    "            \n",
    "            print('ERA-5 data downloaded')\n",
    "\n",
    "        except:\n",
    "            print('No ERA 5 data for this date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b4ca7-faa9-4b9b-9923-27443e059978",
   "metadata": {},
   "source": [
    "### Test removing file from zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93347c00-cac2-4747-9629-e1ac3ee2b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove files from ERA5 zip\n",
    "import subprocess\n",
    "cmd = ['zip', '-d', new_name] + moved_files\n",
    "subprocess.check_call(cmd)\n",
    "subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694127c-d574-4ab4-95a0-28b6eea24d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "z = zipfile.ZipFile(zip_filename)\n",
    "\n",
    "files_to_del = filter( lambda f: f.endswith('exe'), z.namelist()]\n",
    "\n",
    "cmd=['zip', '-d', zip_filename] + files_to_del\n",
    "subprocess.check_call(cmd)\n",
    "\n",
    "# reload the modified archive\n",
    "z = zipfile.ZipFile(zip_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7449c-8ed1-47bc-8ec5-4ac5174e8fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zf.namelist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d7056-fe68-4db8-8f1d-fd567192c1bc",
   "metadata": {},
   "source": [
    "# Download Landcover and DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a44840e-284c-4c70-a37c-717d50dfbba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "landcoverID = '51353'\n",
    "demID = '49506'\n",
    "redID = '49360'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a8f12d-ca90-4031-9018-0a6e66b50211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define areas\n",
    "areas = {\n",
    "    'bbox_1' : [\"46.94599\", \"7.68597\", \"47.20599\", \"7.94597\"],\n",
    "    'bbox_2' : [\"47.20599\",\"7.94597\",\"47.46599\",\"8.20597\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70df1ccf-e238-4399-af75-2dde1350699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = {\n",
    "    'bbox_1' : [\"46\", \"7\", \"46.5\", \"7.5\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f3052-8ab7-489e-a850-78e10e50b001",
   "metadata": {},
   "source": [
    "### Download DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05a08476-69aa-42d1-81f4-453f360759be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "date_dem = datetime(2013, 1, 1, 12)\n",
    "date_red = date(2016,8,3)\n",
    "iso8601 = '%Y-%m-%dT%H:%M:%SZ'\n",
    "\n",
    "bbox = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c90e463-0963-4ec9-b0bb-26478d8bc012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query for DEM --> no time dependence\n",
    "\n",
    "queryDemJson = {\n",
    "    'layers' : [\n",
    "        {            \n",
    "            'type' : 'raster', 'id' : demID,\n",
    "            \"temporal\" : {\"intervals\" : [{\"snapshot\" : date_dem.strftime(iso8601)}]} \n",
    "        },\n",
    "        {   \n",
    "            'type' : 'raster', 'id' : redID,\n",
    "            \"temporal\" : {\"intervals\" : [{\"snapshot\" : date_red.strftime(iso8601)}]}\n",
    "            #, 'output' : False\n",
    "        }\n",
    "    ],\n",
    "    \"spatial\" : {\"type\" : \"square\",  \"coordinates\" : bbox },\n",
    "    'temporal' : {'intervals' : [{'snapshot' : date_dem.strftime(iso8601)}]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b07bac2-1a8d-489b-8a6d-53dc5cb9d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ibmpairs.paw:PAIRS query JSON initialized.\n",
      "WARNING:ibmpairs.paw:Fake submit to PAIRS in order to use (latest) locally cached data.\n",
      "WARNING:ibmpairs.paw:I am sorry, you asked for using the local cache, but your query does not match any existing. I am gonna query PAIRS instead: 'list index out of range'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layers': [{'type': 'raster', 'id': '49506', 'temporal': {'intervals': [{'snapshot': '2013-01-01T12:00:00Z'}]}}, {'type': 'raster', 'id': '49360', 'temporal': {'intervals': [{'snapshot': '2016-08-03T00:00:00Z'}]}}], 'spatial': {'type': 'square', 'coordinates': ['46', '7', '46.5', '7.5']}, 'temporal': {'intervals': [{'snapshot': '2013-01-01T12:00:00Z'}]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ibmpairs.paw:Query successfully submitted, reference ID: 1664856000_32156214\n",
      "INFO:ibmpairs.paw:Here we go, PAIRS query result successfully downloaded as '1664856000_32156214_510f13335410fbd2c2d1bec18e765dcd.zip'.\n",
      "INFO:ibmpairs.paw:Data acknowledgement successfully loaded, print with `self.print_data_acknowledgement()`\n",
      "INFO:ibmpairs.paw:PAIRS meta data loaded from 'output.info'.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA DEM data downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download DEM data\n",
    "\n",
    "for bbox, coord in areas.items():\n",
    "    \n",
    "    PAIRS_SERVER, PAIRS_CREDENTIALS = authenticate(pairs_key)\n",
    "    queryDemJson[\"spatial\"][\"coordinates\"] = coord\n",
    "    print(queryDemJson)\n",
    "\n",
    "    try:    \n",
    "        queryDem = paw.PAIRSQuery(queryDemJson, PAIRS_SERVER, PAIRS_CREDENTIALS, authType='api-key', overwriteExisting=False)\n",
    "        queryDem.submit()\n",
    "\n",
    "        queryDem.poll_till_finished()\n",
    "        queryDem.download()\n",
    "        queryDem.create_layers()\n",
    "\n",
    "        #Check if the Sentinel folder exists\n",
    "        path_Dem = 'downloads/NASA DEM/'\n",
    "        isExistDem = os.path.exists(path_Dem)                          \n",
    "\n",
    "        if isExistDem == True : \n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(path_Dem)\n",
    "\n",
    "        #Rename File\n",
    "        old_name = 'downloads/' + str(queryDem.zipFilePath)\n",
    "        new_name = 'downloads/NASA DEM/' + str(year) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3]) + '.zip'\n",
    "        directory_name = 'downloads/NASA DEM/' + str(year) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3])\n",
    "        os.rename(old_name, new_name)\n",
    "        \n",
    "        #extract zip file\n",
    "        with zipfile.ZipFile(new_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(directory_name)\n",
    "        \n",
    "        #delete the zip file\n",
    "        os.remove(new_name)\n",
    "        \n",
    "        #remove Sentinel 2 bands from the directory\n",
    "        for f in glob.glob(directory_name + \"/High*.*\"):\n",
    "            os.remove(f)\n",
    "        \n",
    "        print('NASA DEM data downloaded')\n",
    "\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5440b-c58e-4f9a-88ba-625e0e916b62",
   "metadata": {},
   "source": [
    "### Download Landcover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b5deab-b2b5-4d57-b5c6-c37693872b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date and space parameters\n",
    "from datetime import date\n",
    "\n",
    "landcover_years = [date(2016, 1, 1), date(2017, 1, 1), date(2018, 1, 1), date(2019, 1, 1)]\n",
    "date_red = date(2016,8,3)\n",
    "date = None\n",
    "iso8601 = '%Y-%m-%dT%H:%M:%SZ'\n",
    "\n",
    "bbox = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aee9672a-3e37-4f11-955d-4b3938d016e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryLCJson = {\n",
    "    'layers' : [\n",
    "        {            \n",
    "            'type' : 'raster', 'id' : landcoverID,\n",
    "            \"temporal\" : {\"intervals\" : [{\"snapshot\" : date}]} \n",
    "        },\n",
    "        {   \n",
    "            'type' : 'raster', 'id' : redID,\n",
    "            \"temporal\" : {\"intervals\" : [{\"snapshot\" : date_red.strftime(iso8601)}]}\n",
    "            #, 'output' : False\n",
    "        }\n",
    "    ],\n",
    "    \"spatial\" : {\"type\" : \"square\",  \"coordinates\" : bbox },\n",
    "    'temporal' : {'intervals' : [{'snapshot' : date }]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6703140a-fe33-49c8-94f6-d6818670baf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying for date 2016-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ibmpairs.paw:PAIRS query JSON initialized.\n",
      "WARNING:ibmpairs.paw:Fake submit to PAIRS in order to use (latest) locally cached data.\n",
      "WARNING:ibmpairs.paw:I am sorry, you asked for using the local cache, but your query does not match any existing. I am gonna query PAIRS instead: 'list index out of range'.\n",
      "INFO:ibmpairs.paw:Query successfully submitted, reference ID: 1664856000_32467882\n",
      "INFO:ibmpairs.paw:Here we go, PAIRS query result successfully downloaded as '1664856000_32467882_381c71d2f8e1a8c64fcca39a8a07124e.zip'.\n",
      "INFO:ibmpairs.paw:Data acknowledgement successfully loaded, print with `self.print_data_acknowledgement()`\n",
      "INFO:ibmpairs.paw:PAIRS meta data loaded from 'output.info'.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landcover data downloaded\n",
      "Trying for date 2017-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ibmpairs.paw:PAIRS query JSON initialized.\n",
      "WARNING:ibmpairs.paw:Fake submit to PAIRS in order to use (latest) locally cached data.\n",
      "WARNING:ibmpairs.paw:I am sorry, you asked for using the local cache, but your query does not match any existing. I am gonna query PAIRS instead: 'list index out of range'.\n",
      "INFO:ibmpairs.paw:Query successfully submitted, reference ID: 1664856000_32555880\n",
      "INFO:ibmpairs.paw:Here we go, PAIRS query result successfully downloaded as '1664856000_32555880_39323cf3574657f69667f4b420e15a5e.zip'.\n",
      "INFO:ibmpairs.paw:Data acknowledgement successfully loaded, print with `self.print_data_acknowledgement()`\n",
      "INFO:ibmpairs.paw:PAIRS meta data loaded from 'output.info'.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landcover data downloaded\n",
      "Trying for date 2018-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ibmpairs.paw:PAIRS query JSON initialized.\n",
      "WARNING:ibmpairs.paw:Fake submit to PAIRS in order to use (latest) locally cached data.\n",
      "WARNING:ibmpairs.paw:I am sorry, you asked for using the local cache, but your query does not match any existing. I am gonna query PAIRS instead: 'list index out of range'.\n",
      "INFO:ibmpairs.paw:Query successfully submitted, reference ID: 1664856000_32694394\n",
      "INFO:ibmpairs.paw:Here we go, PAIRS query result successfully downloaded as '1664856000_32694394_ae23be732599e122042dfc81a5e5cf82.zip'.\n",
      "INFO:ibmpairs.paw:Data acknowledgement successfully loaded, print with `self.print_data_acknowledgement()`\n",
      "INFO:ibmpairs.paw:PAIRS meta data loaded from 'output.info'.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landcover data downloaded\n",
      "Trying for date 2019-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ibmpairs.paw:PAIRS query JSON initialized.\n",
      "WARNING:ibmpairs.paw:Fake submit to PAIRS in order to use (latest) locally cached data.\n",
      "WARNING:ibmpairs.paw:I am sorry, you asked for using the local cache, but your query does not match any existing. I am gonna query PAIRS instead: 'list index out of range'.\n",
      "INFO:ibmpairs.paw:Query successfully submitted, reference ID: 1664856000_32822585\n",
      "INFO:ibmpairs.paw:Here we go, PAIRS query result successfully downloaded as '1664856000_32822585_11ce3bbd8d50c476404c249c2554d7d6.zip'.\n",
      "INFO:ibmpairs.paw:Data acknowledgement successfully loaded, print with `self.print_data_acknowledgement()`\n",
      "INFO:ibmpairs.paw:PAIRS meta data loaded from 'output.info'.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n",
      "WARNING:ibmpairs.paw:GDAL is not available for proper GeoTiff loading, default to standard PIL module to load raster data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landcover data downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download Landcover data\n",
    "\n",
    "for bbox, coord in areas.items():\n",
    "    for year in landcover_years:\n",
    "\n",
    "        print(f'Trying for date {year}')\n",
    "        PAIRS_SERVER, PAIRS_CREDENTIALS = authenticate(pairs_key)\n",
    "\n",
    "        date = year.strftime(iso8601)\n",
    "        queryLCJson[\"layers\"][0][\"temporal\"][\"intervals\"][0][\"snapshot\"] = date\n",
    "        queryLCJson[\"temporal\"][\"intervals\"][0][\"snapshot\"] = date\n",
    "        queryLCJson[\"spatial\"][\"coordinates\"] = coord\n",
    "\n",
    "        \n",
    "        try:    \n",
    "            queryLC = paw.PAIRSQuery(queryLCJson, PAIRS_SERVER, PAIRS_CREDENTIALS, authType='api-key', overwriteExisting=False)\n",
    "            queryLC.submit()\n",
    "\n",
    "            queryLC.poll_till_finished()\n",
    "            queryLC.download()\n",
    "            queryLC.create_layers()\n",
    "\n",
    "            #Check if the Sentinel folder exists\n",
    "            path_LC = 'downloads/LC Copernicus/'\n",
    "            isExistLC = os.path.exists(path_LC)                          \n",
    "\n",
    "            if isExistLC == True : \n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(path_LC)\n",
    "\n",
    "            #Rename File\n",
    "            old_name = 'downloads/' + str(queryLC.zipFilePath)\n",
    "            new_name = 'downloads/LC Copernicus/' + str(year) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3]) + '.zip'\n",
    "            directory_name = 'downloads/LC Copernicus/' + str(year) + '_' + str(coord[0]) + '_' + str(coord[1]) + '_' + str(coord[2]) + '_' + str(coord[3])\n",
    "            os.rename(old_name, new_name)\n",
    "            \n",
    "             #extract zip file\n",
    "            with zipfile.ZipFile(new_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(directory_name)\n",
    "        \n",
    "            #delete the zip file\n",
    "                os.remove(new_name)\n",
    "        \n",
    "            #remove Sentinel 2 bands from the directory\n",
    "            for f in glob.glob(directory_name + \"/High*.*\"):\n",
    "                os.remove(f)\n",
    "            \n",
    "            print('Landcover data downloaded')\n",
    "            \n",
    "        except:\n",
    "            print('No Landcover data for this year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee7681-0a53-4c50-a788-00c8e26f844d",
   "metadata": {},
   "source": [
    "# ------------\n",
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c6b87-8dba-48c9-9605-f9bd2d183b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vpd_layers = [\n",
    "        {     \n",
    "            #Kelvin\n",
    "            \"type\" : \"raster\", \"id\" : \"49423\",\n",
    "            \"aggregation\" : \"Mean\",\n",
    "            \"alias\": \"temperature\",\n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {            \n",
    "            #Kelvin\n",
    "            \"type\" : \"raster\", \"id\" : \"49422\",\n",
    "            \"aggregation\" : \"Mean\",\n",
    "            \"alias\": \"dewpoint\",\n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {\n",
    "            #convert kelvin to rankine\n",
    "            \"alias\": \"temp_rankine\",\n",
    "            \"expression\": \"$temperature * 1.8\"\n",
    "            \n",
    "        },\n",
    "        {\n",
    "            #convert dewpoint kelvin to celcius\n",
    "            \"alias\": \"dewpoint_celcius\",\n",
    "            \"expression\": \"$dewpoint - 273.15\"\n",
    "            \n",
    "        },\n",
    "        {\n",
    "            #calculate saturation\n",
    "            \"alias\": \"saturation\",\n",
    "            \"expression\": \" math:exp(( (math:pow(10,4) * -1.0440397) / $temp_rankine) + -11.29465 + (math:pow(10,-2) * -2.7022355 * $temp_rankine) + (math:pow(10,-5) * 1.289036 * math:pow($temp_rankine,2)) + (math:pow(10,-9) * -2.4780681 * math:pow($temp_rankine,3)) + (6.5459673 * math:log($temp_rankine))) \"\n",
    "        },\n",
    "        {\n",
    "            #convert kelvin to celcius\n",
    "            \"alias\": \"temp_celcius\",\n",
    "            \"expression\": \"$temperature - 273.15\"\n",
    "            \n",
    "        },\n",
    "        {\n",
    "            #calculate relative humidty\n",
    "            \"alias\": \"relative_humidity\",\n",
    "            \"expression\": \"100 * (math:exp((17.625 * $dewpoint_celcius) / (243.04 + $dewpoint_celcius)) / (math:exp((17.625 * $temp_celcius) /(243.04 + $temp_celcius))))\"\n",
    "            \n",
    "        },\n",
    "        {\n",
    "            #calculate partial presurre\n",
    "            \"alias\": \"partial_pressure\",\n",
    "            \"expression\": \"$saturation * ($relative_humidity / 100) \"\n",
    "            \n",
    "        },\n",
    "        {\n",
    "            #calculate vapour pressure deficit\n",
    "            \"alias\": \"vpd\",\n",
    "            \"expression\": \"$saturation * (1-($relative_humidity /100))\"\n",
    "            #or, vpd = $saturation - $partial_pressure\n",
    "            \n",
    "        }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12abbe6-8f2c-4035-a69a-2ef98f6aec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Beni's method (approach 2)\n",
    "\n",
    "# this doesn't work because for specific humidity you need to specify a dimension (aka altitude)\n",
    "\n",
    "vpd_layers = [\n",
    "        {\n",
    "            #convert kelvin to celcius\n",
    "            \"alias\": \"tmaxc\",\n",
    "            \"expression\": \"$MaxT - 273.15\", \n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {            \n",
    "            # Mean specific humidity \n",
    "            \"type\" : \"raster\", \"id\" : \"51513\",\n",
    "            \"aggregation\" : \"Mean\",\n",
    "            \"alias\": \"specifichum\",\n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {\n",
    "            #Pressure\n",
    "            \"type\" : \"raster\", \"id\" : \"49439\",\n",
    "            \"aggregation\" : \"Mean\",\n",
    "            \"alias\": \"pressure\",\n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {\n",
    "            # Calculate the mass mixing ratio of water vapor to dry air \n",
    "            \"alias\": \"wair\",\n",
    "            \"expression\": \"$specifichum / (1 - $specifichum) \",\n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {\n",
    "            #Water vapor pressure\n",
    "            \"alias\": \"eact\",\n",
    "            \"expression\": \"$pressure * $wair * (8.3143/18.02) / ( (8.3143/28.963) + $wair * (8.3143/18.02))\"#,\n",
    "            #\"output\" : \"false\"\n",
    "        },\n",
    "        {\n",
    "            # calculate saturation water vapour pressure in Pa\n",
    "            \"alias\": \"esat\",\n",
    "            \"expression\": \"611*math:exp((17.27 * $tmaxc)/($tmaxc + 237.3))\",\n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {\n",
    "            # Calculate vpd\n",
    "            \"alias\": \"vpd\",\n",
    "            \"expression\": \"$esat - $eact\",\n",
    "            \"output\" : \"false\"\n",
    "        },\n",
    "        {\n",
    "            # Calculate vpd\n",
    "            \"alias\": \"vpd_final\",\n",
    "            \"expression\": \"math:max(0, $vpd)\"\n",
    "        }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought",
   "language": "python",
   "name": "drought"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
